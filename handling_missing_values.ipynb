{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwLUSuvhavVtefatrFejaw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manhhung20033/Learning/blob/main/handling_missing_values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python libraries represent missing numbers as **nan** which is short for \"not a number\". You can detect which cells have missing values, and then count how many there are in each column with the commmad:"
      ],
      "metadata": {
        "id": "z5Ss4HUeXt0D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO1wymLEXlCd"
      },
      "outputs": [],
      "source": [
        "missing_val_count_by_column = (data.isnull().sum())\n",
        "print(missing_val_count_by_column[missing_val_count_by_column > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DROP COLUMN WITH MISSING VALUES"
      ],
      "metadata": {
        "id": "4CSQoRlaYmVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your data is a DataFrame called original_data. You can drop columns with missing values"
      ],
      "metadata": {
        "id": "JvNfstUUZHG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_without_missing_values = original_data.dropna(axis=1)"
      ],
      "metadata": {
        "id": "NYKp051yYtSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many case, you will have both a training dataset and a set dataset. You will want to drop the same columnin both DataFrames."
      ],
      "metadata": {
        "id": "Wqd6qrV-ZW-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_with_missing = [col for col in original_data.columns \n",
        "                                 if original_data[col].isnull().any()]\n",
        "reduced_original_data = original_data.drop(cols_with_missing, axis=1)\n",
        "reduced_test_data = test_data.drop(cols_with_missing, axis=1)"
      ],
      "metadata": {
        "id": "ZQbYF6U0ZlPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A BETTER OPTION: IMPUTATION"
      ],
      "metadata": {
        "id": "uVOu6_eGZyCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation fills in the missing values with some number. It usually use more accurate moldels than dropping the column entirely."
      ],
      "metadata": {
        "id": "kJoiFqDtZ4OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "my_imputer = SimpleImputer()\n",
        "data_with_imputed_values = my_imputer.fit_transform(original_data)"
      ],
      "metadata": {
        "id": "PwRoXkUxZ3BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default behavior fills in the mean values for imputation."
      ],
      "metadata": {
        "id": "wABs4h0gaXje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AN EXTENSION TO IMPUTATION"
      ],
      "metadata": {
        "id": "xHdnhD_Pal_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation is the standard approach and it usually works well. However, imputed values may by systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. Here's how it might look:"
      ],
      "metadata": {
        "id": "2P2Cp5D3atU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make copy to avoid changing original data (when Imputing)\n",
        "new_data = original_data.copy()\n",
        "\n",
        "# make new columns indicating what will be imputed\n",
        "cols_with_missing = (col for col in new_data.columns \n",
        "                                 if new_data[col].isnull().any())\n",
        "for col in cols_with_missing:\n",
        "    new_data[col + '_was_missing'] = new_data[col].isnull()\n",
        "\n",
        "# Imputation\n",
        "my_imputer = SimpleImputer()\n",
        "new_data = pd.DataFrame(my_imputer.fit_transform(new_data))\n",
        "new_data.columns = original_data.columns"
      ],
      "metadata": {
        "id": "zPP_Cck3a-vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The source: [Kaggle-handling missing values](https://www.kaggle.com/code/dansbecker/handling-missing-values/notebook)"
      ],
      "metadata": {
        "id": "22OngL0NbAE1"
      }
    }
  ]
}